{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 3, 4])\n",
      "tensor([[[1.0850, 1.1302, 1.1053, 0.8410],\n",
      "         [0.9896, 0.9749, 0.9435, 0.7341],\n",
      "         [1.1135, 1.1562, 1.0542, 0.8725]],\n",
      "\n",
      "        [[0.9771, 0.8424, 1.0311, 0.9762],\n",
      "         [0.9647, 0.8057, 1.0239, 0.9300],\n",
      "         [1.1297, 0.9856, 1.2481, 1.0853]]])\n",
      "torch.Size([2, 12, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.functional import F\n",
    "from dl_solver import HyperParameters\n",
    "\n",
    "\n",
    "class PatchNet(nn.Module):\n",
    "    hparams: HyperParameters\n",
    "\n",
    "    def __init__(self, hparams: HyperParameters):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # self.temperatures = LearnableTemperatures(hparams)\n",
    "\n",
    "    def _soft_forward_step(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        pos_seq: Tensor,\n",
    "        encoder_memory: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: Tensor[B, num_pieces, num_features]\n",
    "            pos_seq: Tensor[B, num_pieces, 3]\n",
    "            encoder_memory: Optional[Tensor[B, num_pieces, num_features]]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tuple[Tensor, Tensor, Tensor], Tensor]: (pos_seq, (row, col, rot)_logits, encoder_memory)\n",
    "        \"\"\"\n",
    "        x, encoder_memory = self.transformer(x, pos_seq, encoder_memory)\n",
    "        logits = self.classifier(x, *self.hparams.puzzle_shape)\n",
    "\n",
    "        num_rows, num_cols = self.hparams.puzzle_shape\n",
    "\n",
    "        # Enhance logits using the unique selection strategy for 2D structure\n",
    "        enhanced_probs = self._enhance_unique_selection(\n",
    "            logits[0], logits[1], self.hparams.softmax_temperature\n",
    "        )  # This should return [B, L, num_rows, num_cols]\n",
    "\n",
    "        # Applying Gumbel-Softmax on the enhanced probabilities\n",
    "        # Flatten the enhanced probabilities for Gumbel-Softmax if needed\n",
    "        flat_enhanced_probs = enhanced_probs.view(\n",
    "            enhanced_probs.shape[0], enhanced_probs.shape[1], -1\n",
    "        )\n",
    "        probabilities = F.gumbel_softmax(\n",
    "            flat_enhanced_probs, tau=self.hparams.gumbel_temperature, hard=True\n",
    "        )\n",
    "\n",
    "        # Get indices from the probabilities\n",
    "        indices = probabilities.argmax(dim=-1)\n",
    "        row_indices = indices // num_cols  # Integer division to find row index\n",
    "        col_indices = indices % num_cols  # Modulo to find column index\n",
    "\n",
    "        # Rotation logits processed separately\n",
    "        rotation_indices = torch.argmax(logits[2], dim=-1)\n",
    "\n",
    "        # Stack the indices to form the final position sequence tensor\n",
    "        pos_seq = torch.stack([row_indices, col_indices, rotation_indices], dim=-1).to(\n",
    "            torch.float32\n",
    "        )\n",
    "\n",
    "        return pos_seq, logits, encoder_memory\n",
    "\n",
    "    def _enhance_unique_selection(\n",
    "        self, row_logits: Tensor, col_logits: Tensor\n",
    "    ):\n",
    "        \"\"\"Apply self-competition to enhance unique selections\n",
    "\n",
    "        Args:\n",
    "            row_logits: Tensor[\"B, L, num_rows\", torch.float32]\n",
    "            col_logits: Tensor[\"B, L, num_cols\", torch.float32]\n",
    "            temperature (float): Temperature for softmax\n",
    "\n",
    "        Returns:\n",
    "            Tensor [B, L, num_rows, num_cols]: Joint probabilities that have been modified as follows:\n",
    "            - In Case of a Clash, where multiple token have the same highest probability:\n",
    "                - The token that has the highest probability for idx j - the coordinates (row_idx, col_idx)_j -\n",
    "                  it's probability is not modified.\n",
    "                - All other tokens that predicted the same coordinates (row_idx, col_idx)_j are penalized by reducing\n",
    "                  P_j by a factor of hparams.not_unique_penalty = 2; All other logits are increased accordingly by applying softmax again!\n",
    "                  This is applied iteratively until no more clashes are exist!\n",
    "            - All Operations should be differentiable!\n",
    "        \"\"\"\n",
    "        num_rows, num_cols = self.hparams.puzzle_shape\n",
    "\n",
    "        # Compute joint probabilities\n",
    "        joint_probs = self._compute_joint_probabilities(row_logits, col_logits)\n",
    "\n",
    "        # Flatten to [B, num_pieces, num_rows * num_cols]\n",
    "        flat_probs = joint_probs.view(joint_probs.shape[0], joint_probs.shape[1], -1)\n",
    "\n",
    "        # Identify token clashes: check if the maximum probability per class is chosen by more than one token\n",
    "        max_probs, max_indices = flat_probs.max(dim=2, keepdim=True)  # Max probability per token\n",
    "        token_clashes = (flat_probs == max_probs).sum(dim=1) > 1  # More than one token per max probability\n",
    "\n",
    "        # Identify class clashes: check if any class is being overly selected\n",
    "        class_clashes = flat_probs.max(dim=1, keepdim=True)  # Max probability per class across all tokens\n",
    "\n",
    "        # Apply penalties for token clashes\n",
    "        penalties = token_clashes.float() * self.hparams.non_unique_penalty\n",
    "        adjusted_logits = flat_probs - penalties\n",
    "\n",
    "        # Normalize again using softmax\n",
    "        return F.softmax(adjusted_logits, dim=-1).view_as(joint_probs)\n",
    "\n",
    "\n",
    "    def _compute_joint_probabilities(\n",
    "        self, row_logits: Tensor, col_logits: Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            row_logits (Tensor[B, num_pieces, num_rows])\n",
    "            col_logits (Tensor[B, num_pieces, num_cols])\n",
    "            temperature (float, optional): Defaults to 1.0.\n",
    "\n",
    "        Returns:\n",
    "            Tensor[B, num_pieces, num_rows, num_cols]: Joint probabilities\n",
    "        \"\"\"\n",
    "        # Compute probabilities within each token over all classes\n",
    "        row_probs = F.softmax(row_logits, dim=-1)\n",
    "        col_probs = F.softmax(col_logits, dim=-1)\n",
    "\n",
    "        joint_probs = row_probs[:, :, :, None] * col_probs[:, :, None, :]\n",
    "\n",
    "        return joint_probs\n",
    "\n",
    "    def apply_penalties(self, joint_probs: Tensor) -> Tensor:\n",
    "        # Flatten to [B, num_pieces, num_rows * num_cols]\n",
    "        flat_probs = joint_probs.view(*joint_probs.shape[:2], -1)\n",
    "\n",
    "        max_probs_per_token, _ = flat_probs.max(\n",
    "            dim=1, keepdim=True\n",
    "        )  # [B, 1, num_rows * num_cols]\n",
    "        max_probs_per_class, _ = flat_probs.max(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # [B, num_pieces, 1]\n",
    "\n",
    "        # Masks that identify the maximum probability per token / class / globally\n",
    "        max_per_token = (\n",
    "            flat_probs == max_probs_per_token\n",
    "        )  # [B, num_pieces, num_rows * num_cols]\n",
    "        max_per_class = flat_probs == max_probs_per_class\n",
    "\n",
    "        penalty_scale = (\n",
    "            torch.abs(flat_probs - max_probs_per_token)\n",
    "            / max_probs_per_token\n",
    "            * self.hparams.non_unique_penalty\n",
    "        )\n",
    "\n",
    "        # Apply penalties using softmax for differentiability\n",
    "        flat_probs = torch.where(\n",
    "            ~max_per_class & max_per_token,\n",
    "            torch.softmax(flat_probs * penalty_scale, dim=-1),\n",
    "            flat_probs,\n",
    "        )\n",
    "        flat_probs = torch.where(\n",
    "            ~max_per_class & ~max_per_token,\n",
    "            torch.softmax(flat_probs / penalty_scale, dim=-1),\n",
    "            flat_probs,\n",
    "        )\n",
    "        flat_probs = torch.where(\n",
    "            max_per_class & ~max_per_token,\n",
    "            torch.softmax(flat_probs / penalty_scale, dim=-1),\n",
    "            flat_probs,\n",
    "        )\n",
    "        flat_probs = torch.where(\n",
    "            max_per_class & max_per_token,\n",
    "            torch.softmax(flat_probs / penalty_scale, dim=-1),\n",
    "            flat_probs,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (flat_probs + torch.finfo(torch.float32).eps).log().softmax(-1)\n",
    "        ).view_as(joint_probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_unique_indices(spatial_indices: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Check uniqueness of spatial indices within each batch.\n",
    "        Args:\n",
    "            spatial_indices: Tensor[torch.int64] - (B, num_pieces, 2) [row_idx, col_idx]\n",
    "        Returns:\n",
    "            is_unique: Tensor[torch.bool] - (B, num_pieces)\n",
    "        \"\"\"\n",
    "        batch_size, num_pieces = spatial_indices.size(0), spatial_indices.size(1)\n",
    "        unique_mask = torch.ones(\n",
    "            (batch_size, num_pieces), dtype=torch.bool, device=spatial_indices.device\n",
    "        )\n",
    "\n",
    "        # Check each batch independently\n",
    "        for i in range(batch_size):\n",
    "            _, inverse_indices, counts = torch.unique(\n",
    "                spatial_indices[i], dim=0, return_inverse=True, return_counts=True\n",
    "            )\n",
    "            unique_mask[i] = counts[inverse_indices] == 1\n",
    "\n",
    "        return unique_mask\n",
    "\n",
    "model = PatchNet(HyperParameters(num_post_iters=20, non_unique_penalty=0.5))\n",
    "\n",
    "row_logits = torch.rand(2, 12, 3)\n",
    "col_logits = torch.rand(2, 12, 4)\n",
    "\n",
    "joint_probs = model._compute_joint_probabilities(row_logits, col_logits)\n",
    "print(joint_probs.shape)\n",
    "print(joint_probs.sum(1))\n",
    "penalized_probs = model.apply_penalties(joint_probs=joint_probs)\n",
    "print(joint_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 12, 4])\n",
      "tensor([[[0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "rotation_probs = torch.rand((8, 12, 4)).softmax(dim=-1)\n",
    "ret = torch.functional.F.gumbel_softmax(rotation_probs, 1, hard=True).squeeze(\n",
    "            -1\n",
    "        )\n",
    "print(ret.shape)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 4 (input tensor's size at dimension -1), but got split_sizes=[1, 1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m rows, _, cols \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rows \u001b[38;5;241m=\u001b[39m rows\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m cols \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/prj-robotik/lib/python3.12/site-packages/torch/functional.py:195\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    190\u001b[0m         split, (tensor,), tensor, split_size_or_sections, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# call here.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_size_or_sections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prj-robotik/lib/python3.12/site-packages/torch/_tensor.py:921\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_with_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 4 (input tensor's size at dimension -1), but got split_sizes=[1, 1, 1]"
     ]
    }
   ],
   "source": [
    "t = torch.rand(8, 12, 3, 4)\n",
    "rows, _, cols = torch.split(t, [1, 1, 1], dim=-1)\n",
    "rows = rows.squeeze(-1)\n",
    "cols = cols.squeeze(-1)\n",
    "rows.shape, cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 4 (input tensor's size at dimension -1), but got split_sizes=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rows\u001b[38;5;241m.\u001b[39mshape, cols\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/prj-robotik/lib/python3.12/site-packages/torch/functional.py:195\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    190\u001b[0m         split, (tensor,), tensor, split_size_or_sections, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# call here.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_size_or_sections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prj-robotik/lib/python3.12/site-packages/torch/_tensor.py:921\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_with_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 4 (input tensor's size at dimension -1), but got split_sizes=[1, 1]"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 12, 2]), torch.Size([8, 12, 1]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def softargmax2d(input, beta=100):\n",
    "    *_, h, w = input.shape\n",
    "\n",
    "    input = input.reshape(*_, h * w)\n",
    "    input = nn.functional.softmax(beta * input, dim=-1)\n",
    "\n",
    "    indices_c, indices_r = torch.meshgrid(\n",
    "        torch.linspace(0, 1, w, device=input.device),\n",
    "        torch.linspace(0, 1, h, device=input.device),\n",
    "    )\n",
    "\n",
    "    indices_r = indices_r.reshape(-1, h * w)\n",
    "    indices_c = indices_c.reshape(-1, h * w)\n",
    "\n",
    "    result_r = torch.sum((h - 1) * input * indices_r, dim=-1)\n",
    "    result_c = torch.sum((w - 1) * input * indices_c, dim=-1)\n",
    "\n",
    "    result = torch.stack([result_r, result_c], dim=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def softargmax1d(input, beta=100):\n",
    "    *_, n = input.shape\n",
    "    input = nn.functional.softmax(beta * input, dim=-1)\n",
    "    indices = torch.linspace(0, 1, n)\n",
    "    result = torch.sum((n - 1) * input * indices, dim=-1)\n",
    "    return result\n",
    "\n",
    "pos_probs = softargmax2d(torch.rand(8, 12, 3, 4))\n",
    "rot_probs = softargmax1d(torch.rand(8, 12, 4))\n",
    "\n",
    "pos_probs.shape, rot_probs.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seq = torch.cat([pos_probs, rot_probs.unsqueeze(-1)], dim=-1)\n",
    "pos_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joint_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flat_joint_probs \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_probs\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mjoint_probs\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m token_to_class \u001b[38;5;241m=\u001b[39m flat_joint_probs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m class_to_token \u001b[38;5;241m=\u001b[39m flat_joint_probs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joint_probs' is not defined"
     ]
    }
   ],
   "source": [
    "flat_joint_probs = joint_probs.view(*joint_probs.shape[:2], -1)\n",
    "token_to_class = flat_joint_probs.argmax(1)\n",
    "class_to_token = flat_joint_probs.argmax(-1)\n",
    "\n",
    "token_to_class, class_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max=flat_joint_probs.argmax(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indices = flat_joint_probs.argmax(dim=1, keepdim=True)\n",
    "is_max = torch.zeros_like(flat_joint_probs).scatter_(1, max_indices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(joint_probs: Tensor, dim: int) -> Tensor:\n",
    "    max_indices = flat_joint_probs.argmax(dim=dim, keepdim=True)\n",
    "    return torch.zeros_like(flat_joint_probs).scatter_(dim, max_indices, 1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_flat_joint_prons = flat_joint_probs.clone()\n",
    "flat_joint_probs[~get_max(flat_joint_probs, 1)] *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4,  7,  0,  6,  5,  3,  1,  2,  8,  9, 11, 10],\n",
       "         [ 3,  2, 11, 10,  0,  1,  8,  9,  7,  6,  5,  4],\n",
       "         [ 5,  4,  1,  0,  9,  7,  8,  6,  3, 11,  2, 10],\n",
       "         [ 8,  0, 11,  4,  9,  7,  3,  1, 10,  5,  2,  6],\n",
       "         [ 1,  3,  9, 11,  2,  5,  7, 10,  0,  6,  8,  4],\n",
       "         [ 3,  2,  7,  0,  1, 11,  6, 10,  9,  5,  8,  4],\n",
       "         [ 8, 10,  9, 11,  4,  7,  5,  6,  1,  0,  2,  3],\n",
       "         [ 9, 11,  7, 10,  5,  6,  8,  1,  3,  4,  2,  0],\n",
       "         [ 3, 11,  0,  8,  1,  7,  9,  2, 10,  4,  5,  6],\n",
       "         [10,  8,  2,  6,  0,  4,  9, 11,  1,  5,  7,  3],\n",
       "         [ 6,  5, 10,  7,  4,  9,  2,  1, 11,  8,  3,  0],\n",
       "         [ 0,  2,  6,  4,  1,  5, 10,  8,  9,  3,  7, 11]],\n",
       "\n",
       "        [[ 4,  5,  7,  0,  8,  6,  1,  9,  3, 11, 10,  2],\n",
       "         [ 9, 11, 10,  5,  7,  3,  8,  1,  6,  4,  2,  0],\n",
       "         [ 7,  5,  6, 11,  4,  9,  3,  1, 10,  2,  8,  0],\n",
       "         [ 1,  5,  3,  7,  9,  2,  0,  6, 11,  4,  8, 10],\n",
       "         [ 2, 10,  1,  0,  8,  9,  6,  3,  5,  4, 11,  7],\n",
       "         [ 7, 11,  3,  8,  6,  4, 10,  0,  2,  5,  9,  1],\n",
       "         [ 0,  3,  8,  2,  1,  4, 11, 10,  7,  6,  9,  5],\n",
       "         [ 9,  1,  8, 11,  3,  0, 10,  2,  5,  7,  4,  6],\n",
       "         [ 6,  2,  4,  0, 10,  8,  7,  5,  3,  1, 11,  9],\n",
       "         [10,  8, 11,  9,  6,  7,  2,  3,  4,  0,  5,  1],\n",
       "         [ 3,  0,  2,  1,  4,  7,  8,  6,  5, 11, 10,  9],\n",
       "         [ 1,  9,  2, 10,  5,  3, 11,  8,  0,  6,  7,  4]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = flat_joint_probs.sort(dim=-1, descending=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax_argmax(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Gumbel-Softmax (a.k.a Concrete) approximation of argmax.\n",
    "    This function returns the index as a one-hot encoded vector.\n",
    "    \"\"\"\n",
    "    y_soft = F.gumbel_softmax(logits, tau=temperature, hard=True, dim=-1)\n",
    "    # _, index = y_soft.max(dim=-1)\n",
    "    # y_hard = torch.zeros_like(logits).scatter_(-1, index.unsqueeze(-1), 1.0)\n",
    "    # y = y_hard - y_soft.detach() + y_soft\n",
    "    return y_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_softmax_argmax(flat_joint_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  3,  5,  8,  1,  3,  8,  9,  3, 10,  6,  0],\n",
       "        [ 4,  9,  7,  1,  2,  7,  0,  9,  6, 10,  3,  1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_joint_probs.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj-robotik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
