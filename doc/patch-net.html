<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>DL-README</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<h1 id="patch-net-deep-learning-for-jigsaw-puzzle-solving">PATCH-Net: Deep Learning for Jigsaw Puzzle Solving</h1>
<h2 id="overview">Overview</h2>
<p><strong>PATCH-Net</strong> (<strong>P</strong>uzzle <strong>A</strong>ssembly by <strong>T</strong>ransformer and <strong>C</strong>NN <strong>H</strong>ybrid <strong>Net</strong>work) utilizes a pre-trained CNN backbone for feature extraction, an optional method for dimensionality reduction of the CNNs latent space, and a subsequent Transformer architecture with 2D/3D Learnable Fourier Features for spatial encoding. The model predicts the position and orientation of puzzle pieces, with three classification heads.</p>
<h2 id="other-md-files">Other MD Files</h2>
<ul>
<li><a href="TODO.md">TODO.md</a></li>
<li><a href="DATA-PIECEMAKER.md">DATA-PIECEMAKER.md</a>: Information about the submodule <code>piecemaker</code> for jigsaw puzzle generation, and the data structure of the original and puzzle-ized dataset.</li>
</ul>
<h2 id="architecture">Architecture</h2>
<figure>
<img src="./.doc-assets/patch-net.svg" alt="" /><figcaption>patch-net.svg</figcaption>
</figure>
<ul>
<li><strong>Input</strong>: Images represented as <span class="math inline"><strong>X</strong> ∈ ℝ<sup><em>L</em> × <em>C</em><sub>RGB</sub> × <em>H</em> × <em>W</em></sup></span></li>
<li><strong>CNN</strong>: A fine-tuned backbone (ResNet, EfficientNet) for feature extraction.
<ul>
<li>A good starting-point for model selection might be <a href="https://ar5iv.labs.arxiv.org/html/2310.02037">An evaluation of pre-trained models for feature extraction in image classification</a>, <a href="https://arxiv.org/abs/2310.02037">Paper</a></li>
</ul></li>
<li>(optional) <strong>Dimensionality Reduction</strong>: PCA or Autoencoders reduce the feature space dimensionality.</li>
<li><strong>Transformer</strong>:
<ul>
<li>Encoder processes features without positional embedding.</li>
<li>Decoder uses 2D/3D <a href="https://arxiv.org/pdf/2106.02795v1">Learnable Fourier Features</a> for spatial encoding.</li>
</ul></li>
<li><strong>Output</strong>: Predicts position <span class="math inline">(<em>x</em>, <em>y</em>)</span> and orientation <span class="math inline"><em>φ</em></span></li>
</ul>
<p><br /><span class="math display">(<em>x</em>, <em>y</em>, <em>φ</em>),  where <em>x</em> ∈ {0, …, <em>N</em> − 1}, <em>y</em> ∈ {0, …, <em>M</em> − 1}, <em>φ</em> ∈ {<em>k</em> ⋅ 90|<em>k</em> ∈ 0, …, 3}</span><br /></p>
<ul>
<li><strong>Classification Heads</strong>: Three heads (<code>nn.Linear</code>) for predicting the location and orientation of puzzle pieces. Handling varying numbers of puzzle pieces is a future consideration.</li>
<li><strong>Loss Calculation</strong>: Combines <span class="math inline">ℒ<sub>2</sub></span> for position and <span class="math inline">ℒ<sub>CE</sub></span> for orientation.</li>
</ul>
<p><br /><span class="math display">ℒ<sub>total
</sub> = ∥<strong>Y</strong><sub>:,:, : 2</sub> − <strong>Ŷ</strong><sub>:,:, : 2</sub>∥<sub><em>F</em></sub><sup>2</sup> + <em>η</em>ℒ<sub>CE</sub>(<strong>Y</strong><sub>:,:,2</sub>, <strong>Ŷ</strong><sub>:,:,2</sub>)</span><br /> where <span class="math inline"><em>η</em> ∈ ℝ<sup>+</sup></span> is a hyperparameter.</p>
<h2 id="considerations">Considerations</h2>
<ul>
<li><strong>Data Representation</strong>: <span class="math inline"><strong>y</strong> ∈ {0, …, <em>N</em><sub>max</sub>} × {0, …, <em>M</em><sub>max</sub>} × {0, …3}</span></li>
<li><strong>Concatenation/Addition in Transformer Decoder</strong>: <span class="math inline">$\widetilde{\mathbf{X}}^{\langle t\rangle} \gets \widetilde{\mathbf{X}}^{\langle t-1\rangle}_{:,:,:d_{\text{model}}} \oplus \mathbf{E}_{\text{P}}$</span></li>
<li><strong>Dimensionality Reduction</strong>: May occur after the dense layer for enhanced performance.</li>
<li><strong>Unique Labels Check</strong>: Ensures no repetition of puzzle piece positions.</li>
<li><strong>Beam Search</strong>: To improve the decoder’s predictions.</li>
<li>It might be a more straightforward approach to employ a pre-trained ViT model instead of a pre-trained CNN <span class="math inline">⊕</span> custom Transformer architecture.</li>
</ul>
<hr />
<h2 id="third-party-libraries">Third-Party Libraries</h2>
<h3 id="jigsaw-puzzle-generation">Jigsaw Puzzle Generation</h3>
<ul>
<li><p><a href="https://github.com/jkenlooper/piecemaker/tree/main?tab=readme-ov-file">GitHub::piecemaker</a></p></li>
<li><p><a href="lib/piecemaker">location</a> of the submodule</p></li>
<li><p>find further information (installation, usage, …) in <a href="./DATA-PIECEMAKER.md">DATA-PIECEMAKER.md</a></p></li>
<li><p><strong>Data Augmentation</strong>: <a href="https://albumentations.ai/">Albumentations</a></p></li>
<li><p><strong>DL-Framework</strong>: <a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a></p></li>
</ul>
<h2 id="literature-resources">Literature &amp; Resources</h2>
<ul>
<li><strong>Dataset</strong>: <a href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data">ImageNet</a></li>
<li><a href="https://arxiv.org/pdf/2106.02795v1">Learnable Fourier Features for Spatially Encoded Transformers</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0167865522003920">Jigsaw-ViT: Learning jigsaw puzzles in vision transformer</a>, <a href="https://github.com/yingyichen-cyy/JigsawViT/tree/master">GitHub</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
<li><a href="https://www.youtube.com/watch?v=wjZofJX0v4M">Intuition for Embeddings</a></li>
</ul>
<h1 id="todos">TODOs</h1>
<h2 id="data-generation">Data Generation</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Explore <a href="%5Bpiecemaker%5D(https://github.com/jkenlooper/piecemaker/tree/main?tab=readme-ov-file)">piecemaker</a> parameters to generate puzzles resembling real-world scenarios.</li>
<li><input type="checkbox" disabled="" />
Select parameters for puzzles that exhibit more common Jigsaw puzzle properties / characteristics.</li>
<li><input type="checkbox" disabled="" />
Implement parallel processing in <code>piecemaker</code>.</li>
<li><input type="checkbox" disabled="" />
Generate a dataset of 1,000 images from <a href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data">ImgNet</a>.</li>
<li><input type="checkbox" disabled="" />
Choose a suitable format / data structure for the labels.</li>
<li><input type="checkbox" disabled="" />
Store the dataset in a more efficient format (e.g. HDF5), to enable batched loading.</li>
</ul>
<p><br /><span class="math display">(<em>x</em>, <em>y</em>, <em>φ</em>),  where <em>x</em> ∈ {0, …, <em>M</em>}, <em>y</em> ∈ {0, …, <em>M</em>}, <em>φ</em> ∈ {<em>k</em> ⋅ 90|<em>k</em> ∈ 0, …, 3}</span><br /></p>
<h3 id="imagenet">Imagenet</h3>
<ul>
<li>We should adhere to the dataset order as per the <code>Loc_&lt;split&gt;_solution.csv</code> files. Meaning: first line –&gt; first sample</li>
<li>We should transfer the following information into our Jigsaw-Imagenet dataset:
<ul>
<li>ImageID</li>
<li>Class ID (s)</li>
<li>Number of bbox predictions</li>
</ul></li>
<li>We might onyl use the cropped bbox images for the Jigsaw puzzles. And maybe only those that have a certain minimum size.</li>
<li>In the Jigsaw-Imagenet dataset, we should store the separate pieces in a single file to avoid loading multiple files for each sample. consider formats: <code>HDF5</code> or <code>Parquet</code>.</li>
<li>We might even save multiple samples in a single file to simplify the data loading process.</li>
</ul>
<hr />
<h2 id="model-research">Model Research</h2>
<h3 id="cnn">CNN</h3>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Choose an appropriate pre-trained CNN as the backbone.<ul>
<li>Candidates:</li>
<li><code>ResNet</code></li>
<li><code>EfficientNet</code></li>
</ul></li>
</ul>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Investigate (pre-trained ?) auto-encoders (or PCA) for efficient dimensionality of the CNN’s latent space. Is this necessary?</li>
</ul>
<h3 id="transformer-positional-embeddings">Transformer &amp; Positional Embeddings</h3>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Determine the input / output format for the Transformer Decoder.</li>
<li><input type="checkbox" disabled="" />
Should we employ beam-search for the Transformer Decoder?</li>
<li><input type="checkbox" disabled="" />
Explore handling a variable number of puzzle pieces.</li>
<li><input type="checkbox" disabled="" />
Employ Beam Search?</li>
</ul>
<h4 id="positional-embeddings">Positional Embeddings</h4>
<ul>
<li><input type="checkbox" disabled="" />
Decide between concatenating or adding positional embeddings.</li>
<li>The Transformer Decoders Vocabulary should be the set of all possible piece positions and rotations.</li>
</ul>
<hr />
<h2 id="model-implementation">Model Implementation</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Develop an augmentation pipeline using <a href="https://albumentations.ai/">Albumentations</a>.</li>
<li><input type="checkbox" disabled="" />
<a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a> Framework</li>
</ul>
<h1 id="data-piecemaker">Data-Piecemaker</h1>
<h2 id="piecemaker">Piecemaker</h2>
<ul>
<li><a href="https://github.com/jkenlooper/piecemaker/tree/main?tab=readme-ov-file">GitHub::piecemaker</a></li>
<li><a href="lib/piecemaker">location</a> of the submodule</li>
</ul>
<h3 id="installation">Installation</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="fu">sudo</span> apt-get -y install libspatialindex6</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="fu">sudo</span> apt-get -y install optipng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="fu">sudo</span> apt-get -y install potrace</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="ex">%</span> activate conda / python environment</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="bu">cd</span> lib/piecemaker</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a><span class="ex">pip</span> install --upgrade --upgrade-strategy eager -e .</span></code></pre></div>
<h3 id="folder-structure">Folder Structure</h3>
<ul>
<li><p><code>index.json</code>: This file contains metadata about the puzzle, such as the total number of pieces, the puzzle’s height and width, and the size of each piece.</p>
<ul>
<li><code>piece_cut_variant</code>: The style of the puzzle piece cuts.</li>
<li><code>full_size</code>: The full size of the puzzle.</li>
<li><code>sizes</code>: An array of sizes for the puzzle.</li>
<li><code>sides</code>: An array of sides for the puzzle.</li>
<li><code>piece_count</code>: The total number of pieces in the puzzle.</li>
<li><code>image_author</code>: The author of the image used to generate the puzzle.</li>
<li><code>image_link</code>: The link to the image used to generate the puzzle.</li>
<li><code>image_title</code>: The title of the image used to generate the puzzle.</li>
<li><code>image_description</code>: The description of the image used to generate the puzzle.</li>
<li><code>image_width</code>: The width of the image used to generate the puzzle.</li>
<li><code>image_height</code>: The height of the image used to generate the puzzle.</li>
<li><code>outline_bbox</code>: The bounding box of the puzzle outline.</li>
<li><code>puzzle_author</code>: The author of the puzzle.</li>
<li><code>puzzle_link</code>: The link to the puzzle.</li>
<li><code>table_width</code>: The width of the table on which the puzzle is displayed.</li>
<li><code>table_height</code>: The height of the table on which the puzzle is displayed.</li>
<li><code>piece_properties</code>: An array of objects, each representing a puzzle piece. Each object has the following properties:
<ul>
<li><code>id</code>: The ID of the piece.</li>
<li><code>x</code> and <code>y</code>: The x and y coordinates of the piece on the table.</li>
<li><code>ox</code> and <code>oy</code>: The original x and y coordinates of the piece in the image.</li>
<li><code>r</code>: The rotation of the piece.</li>
<li><code>s</code>: The scale of the piece.</li>
<li><code>w</code> and <code>h</code>: The width and height of the piece.</li>
<li><code>rotate</code>: The rotation of the piece.</li>
<li><code>g</code>: The group of the piece.</li>
</ul></li>
</ul></li>
<li><p><code>adjacent.json</code>: This file contains information about which pieces are adjacent to each other in the original image.</p></li>
<li><p><code>size-100/</code>: This directory contains data for the puzzle pieces when the puzzle size is set to 100.</p>
<ul>
<li><p><code>pieces.json</code>: This file contains information about each piece, including its row and column index in the original image, its rotation (in degrees), and its height and width.</p></li>
<li><p><code>sprite_with_padding_layout.json</code> and <code>sprite_without_padding_layout.json</code>: These files contain information about how the pieces are laid out in the sprite image.</p></li>
<li><p><code>masks.json</code> and <code>piece_id_to_mask.json</code>: These files contain information about the masks used to extract each piece from the original image.</p></li>
<li><p><code>data_uri/</code>: This directory contains Base64 encoded images of each piece.</p></li>
<li><p><code>mask</code>: Contains the unique masks as <code>bmp</code> files.</p></li>
<li><p><code>raster</code>: Contains the N segemented pieces of the Jigsaw puzzle as <code>jpg</code> files.</p></li>
<li><p><code>raster_with_padding</code>: Contains the N segemented pieces of the Jigsaw puzzle as <code>jpg</code> files with padding to rectangular shape.</p></li>
</ul></li>
</ul>
<hr />
<h2 id="original-imagenet-dataset">Original ImageNet Dataset</h2>
<p>can be found <a href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data">here</a></p>
<h3 id="folder-structure-1">Folder Structure</h3>
<ol type="1">
<li><strong>Class Labels</strong>: The class labels are located in the <code>LOC_synset_mapping.txt</code> file. Each line in this file likely contains a class ID and its corresponding label. Example lines: <code>n01440764 tench, Tinca tinca     n01443537 goldfish, Carassius auratus     n01484850 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias</code></li>
<li><strong>Splits</strong>: The <code>train</code>, <code>val</code> &amp; <code>test</code> set images are located in <code>.data/imagenet/ILSVRC/Data/CLS-LOC/&lt;split&gt;</code>.
<ul>
<li><code>train-folder</code>: Contains the images in subfolders named after the class labels. For example, the first directory is <code>n01440764</code>, and contains images of tench fish. The labels are also provided via <code>.data/imagenet/LOC_train_solution.csv</code> - same format as the validation set.</li>
<li><code>val-folder</code>: Contains the images directly as <code>ILSVRC2012_val_&lt;id&gt;.JPEG</code>. The labels are located in <code>.data/imagenet/LOC_val_solution.csv</code> and look like this: <code>csv      ImageID                , PredictionString      ILSVRC2012_val_00008726,n02119789 255 142 454 329 n02119789 44 21 322 295                              &lt;class id&gt;&lt;BBX          &gt; &lt;class id&gt;&lt;BBX        &gt;</code> This example contains two predictions of the same class.</li>
<li><code>test-folder</code>: Containes the images directly as <code>ILSVRC2012_test_&lt;id&gt;.JPEG</code></li>
</ul></li>
</ol>
<p><strong>Consideration</strong>: - We should adhere to the dataset order as per the <code>Loc_&lt;split&gt;_solution.csv</code> files. Meaning: first line –&gt; first sample - We should transfer the following information into our Jigsaw-Imagenet dataset: - ImageID - Class ID (s) - Number of bbox predictions - We might onyl use the cropped bbox images for the Jigsaw puzzles. And maybe only those that have a certain minimum size. - In the Jigsaw-Imagenet dataset, we should store the separate pieces in a single file to avoid loading multiple files for each sample. consider formats: <code>HDF5</code> or <code>Parquet</code>. - We might even save multiple samples in a single file to simplify the data loading process.</p>
</body>
</html>
